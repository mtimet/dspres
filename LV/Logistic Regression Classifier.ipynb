{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "[link](https://www.kaggle.com/c/sf-crime/data?test.csv.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 s, sys: 222 ms, total: 3.28 s\n",
      "Wall time: 3.31 s\n",
      "CPU times: user 2.71 s, sys: 203 ms, total: 2.91 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "#Load Data with pandas, and parse the first column into datetime\n",
    "train_csv = r'data/train.csv'\n",
    "%time train=pd.read_csv(train_csv, parse_dates = ['Dates'])\n",
    "\n",
    "test_csv = r'data/test.csv'\n",
    "%time test=pd.read_csv(test_csv, parse_dates = ['Dates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "encodedCategory = labelEncoder.fit_transform(train.Category)\n",
    "\n",
    "encodedDays = pd.get_dummies(train.DayOfWeek)\n",
    "encodedDistrict = pd.get_dummies(train.PdDistrict)\n",
    "trainDf = pd.concat([encodedDays, encodedDistrict], axis=1)\n",
    "\n",
    "trainDf['Y']=train['Y']\n",
    "trainDf['X']=train['X']\n",
    "trainDf['Category']=encodedCategory\n",
    "\n",
    "encodedDays = pd.get_dummies(test.DayOfWeek)\n",
    "encodedDistrict = pd.get_dummies(test.PdDistrict)\n",
    "testDf = pd.concat([encodedDays, encodedDistrict], axis=1)\n",
    "\n",
    "testDf['Y']=test['Y']\n",
    "testDf['X']=test['X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['Y', 'X']\n",
    "features = np.append(features, train.DayOfWeek.unique())\n",
    "features = np.append(features, train.PdDistrict.unique())\n",
    "features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logisticRegressionClassifier (df, size, features):\n",
    "    # Split arrays or matrices into random train and test subsets\n",
    "    training, validation = train_test_split(df, train_size=size)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(training[features], training['Category'])\n",
    "    predicted = model.predict_proba(validation[features])\n",
    "\n",
    "    print('(size, log_loss) : ',(size,log_loss(validation['Category'], np.array(predicted))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(size, log_loss) :  (0.7, 2.6118485124038018)\n",
      "CPU times: user 2min 35s, sys: 3.9 s, total: 2min 39s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%time logisticRegressionClassifier(trainDf,0.7,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross Validation with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crossValidation (df, size ,features, cvGen=3):\n",
    "    # Split arrays or matrices into random train and test subsets\n",
    "    training, validation = train_test_split(df, train_size=size)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    log_loss = cross_validation.cross_val_score(model,training[features],training['Category'],cv=cvGen,scoring='log_loss')\n",
    "    \n",
    "    print('(size, cv,  log_loss) : ',(size, cvGen, log_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(size, cv,  log_loss) :  (0.7, 3, array([-2.61355628, -2.61416839, -2.61262285]))\n",
      "CPU times: user 5min 10s, sys: 6.82 s, total: 5min 17s\n",
      "Wall time: 5min 20s\n",
      "(size, cv,  log_loss) :  (0.7, 4, array([-2.6132108 , -2.61325661, -2.61452945, -2.61337517]))\n",
      "CPU times: user 7min 57s, sys: 7.43 s, total: 8min 5s\n",
      "Wall time: 8min 14s\n"
     ]
    }
   ],
   "source": [
    "%time crossValidation(trainDf,0.7,features)  #cv = 3\n",
    "%time crossValidation(trainDf,0.7,features,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
